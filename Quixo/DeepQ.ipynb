{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from game import Game\n",
        "from players import *\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "404st0S6QsVu"
      },
      "source": [
        "# Train.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTvhrAldQvBB"
      },
      "source": [
        "IN this section we would like to decide the player neural network architecture and then we would like to train it against a random player and see how it performors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_9kHBZ6-Q4cr"
      },
      "outputs": [],
      "source": [
        "from numpy import unravel_index\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "class From_Pos_Net(torch.nn.Module):\n",
        "    def __init__(self, gamma=0.9):\n",
        "        super().__init__()\n",
        "        self.optimizer = None\n",
        "        self.gamma = gamma\n",
        "        self.p1 = nn.Linear(\n",
        "            26, 100\n",
        "        )  # 26 because we have 25 integers and 1 int for player id\n",
        "        self.p2 = nn.Linear(100, 100)\n",
        "        self.p3 = nn.Linear(100, 100)\n",
        "        self.p4 = nn.Linear(100, 25)\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
        "        x = F.relu(self.p1(x))\n",
        "        x = F.relu(self.p2(x))\n",
        "        x = F.relu(self.p3(x))\n",
        "        from_pos = F.softmax(self.p4(x), dim=-1)\n",
        "\n",
        "        # During the train step he should learn to take the from_pos using the professor convention\n",
        "\n",
        "        return from_pos\n",
        "\n",
        "    def train_net(self, state, next_state, from_pos, target):\n",
        "        if target != 1:\n",
        "            target += self.gamma * torch.max(self.forward(next_state))\n",
        "\n",
        "        output = self.forward(state)\n",
        "        target_f = output.clone()\n",
        "        target_f[np.argmax(from_pos)] = target\n",
        "        target_f.detach()\n",
        "        self.optimizer.zero_grad()\n",
        "        loss = F.mse_loss(output, target_f)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "JhJPnB8TLIz5"
      },
      "outputs": [],
      "source": [
        "class Action_Net(torch.nn.Module):\n",
        "    def __init__(self, gamma=0.9):\n",
        "        super().__init__()\n",
        "        self.optimizer = None\n",
        "        self.gamma = gamma\n",
        "        # network for the action\n",
        "        self.a1 = nn.Linear(27, 50)\n",
        "        self.a2 = nn.Linear(50, 50)\n",
        "        self.a3 = nn.Linear(50, 25)\n",
        "        self.a4 = nn.Linear(25, 4)\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    def forward(self, y):\n",
        "        y = torch.tensor(y, dtype=torch.float32).to(self.device)\n",
        "        y = F.relu(self.a1(y))\n",
        "        y = F.relu(self.a2(y))\n",
        "        y = F.relu(self.a3(y))\n",
        "        act = F.softmax(self.a4(y), dim=-1)\n",
        "\n",
        "        return act\n",
        "\n",
        "    def train_net(self, state, next_state, action, target):\n",
        "        if target != 1:\n",
        "            target += self.gamma * torch.max(self.forward(next_state))\n",
        "\n",
        "        output = self.forward(state)\n",
        "        target_f = output.clone()\n",
        "        target_f[np.argmax(action)] = target\n",
        "        target_f.detach()\n",
        "        self.optimizer.zero_grad()\n",
        "        loss = F.mse_loss(output, target_f)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "dXI27s1FRuDg"
      },
      "outputs": [],
      "source": [
        "class DeepQTrain(Player):\n",
        "    def __init__(self, params):\n",
        "        self._epsilon = 0.3\n",
        "        self.train_mode = True\n",
        "        self.file_name = \"\"\n",
        "        self.learning_rate = params[\"learning_rate\"]\n",
        "        self.weight_path = params[\"weight_path\"]\n",
        "        self.memory = []\n",
        "        self.from_pos_net = From_Pos_Net()\n",
        "        self.from_pos_net.optimizer = optim.Adam(\n",
        "            self.from_pos_net.parameters(),\n",
        "            weight_decay=0,\n",
        "            lr=self.learning_rate,\n",
        "        )\n",
        "        self.action_net = Action_Net()\n",
        "        self.action_net.optimizer = optim.Adam(\n",
        "            self.action_net.parameters(), weight_decay=0, lr=self.learning_rate\n",
        "        )\n",
        "\n",
        "        DEVICE = \"cpu\"\n",
        "        self.from_pos_net.to(DEVICE)\n",
        "        self.action_net.to(DEVICE)\n",
        "\n",
        "    def set_epsilon(self, eps: int) -> None:\n",
        "        self._epsilon = eps\n",
        "\n",
        "    def get_epsilon(self) -> int:\n",
        "        return self._epsilon\n",
        "\n",
        "    def make_move(self, game: \"Game\") -> tuple[tuple[int, int], Move]:\n",
        "        input = game.get_board()\n",
        "        from_pos_net_input = np.append(input, game.get_current_player())\n",
        "\n",
        "        if self.train_mode and np.random.random() < self._epsilon:\n",
        "            possible_moves = game.get_possible_moves()\n",
        "            index = np.random.choice(len(possible_moves))\n",
        "            move = possible_moves[index]\n",
        "        else:\n",
        "            from_pos = self.from_pos_net(from_pos_net_input)\n",
        "            from_pos = from_pos.reshape(5, 5)  # we reshape it in matrix form.\n",
        "            from_pos = unravel_index(\n",
        "                from_pos.cpu().argmax(), from_pos.shape\n",
        "            )  # we take the position of the matrix we are interested into.\n",
        "            # this will be part of the additional input to the action net.\n",
        "            act_net_input = np.append(input, from_pos)\n",
        "            act = self.action_net.forward(act_net_input)\n",
        "            act = act.cpu().argmax().numpy()\n",
        "\n",
        "            move = (from_pos, Move(act))\n",
        "\n",
        "        # vorrei la memoria fatta da (board_state+player, (move, act))\n",
        "        self.memory.append((from_pos_net_input, move))\n",
        "\n",
        "        return move\n",
        "\n",
        "    def back_prop(self, reward: int):\n",
        "        for idx in range(len(self.memory) - 1):\n",
        "            curr_state, move = self.memory[idx]\n",
        "            curr_pos, curr_move = move\n",
        "            next_state, n_move = self.memory[idx + 1]\n",
        "            next_pos, _ = n_move\n",
        "            r = 0 if idx < len(self.memory) - 2 else reward\n",
        "            self.from_pos_net.train_net(curr_state, next_state, curr_pos, r)\n",
        "            self.action_net.train_net(\n",
        "                np.append(curr_state[:-1], curr_pos),\n",
        "                np.append(next_state[:-1], next_pos),\n",
        "                curr_move,\n",
        "                r,\n",
        "            )\n",
        "\n",
        "        self.memory = []\n",
        "\n",
        "    def save_policy(self):\n",
        "        from_pos_weights = self.from_pos_net.state_dict()\n",
        "        act_weights = self.action_net.state_dict()\n",
        "        torch.save(from_pos_weights, \"from_pos_net.h5\")\n",
        "        torch.save(act_weights, \"action_net.h5\")\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLHlE5QRlHOt"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### New Game subclass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_Rwuezm3GVzc"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "K7uFCZmLSw7C"
      },
      "outputs": [],
      "source": [
        "class GameTrainer(Game):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "    def print(self) -> None:\n",
        "        # os.system(\"cls||clear\")\n",
        "        pass\n",
        "\n",
        "    def __acceptable_slides(self, from_position: tuple[int, int]):\n",
        "        \"\"\"When taking a piece from {from_position} returns the possible moves (slides)\"\"\"\n",
        "        acceptable_slides = [Move.BOTTOM, Move.TOP, Move.LEFT, Move.RIGHT]\n",
        "        axis_0 = from_position[0]  # axis_0 = 0 means uppermost row\n",
        "        axis_1 = from_position[1]  # axis_1 = 0 means leftmost column\n",
        "\n",
        "        if axis_0 == 0:  # can't move upwards if in the top row...\n",
        "            acceptable_slides.remove(Move.TOP)\n",
        "        elif axis_0 == 4:\n",
        "            acceptable_slides.remove(Move.BOTTOM)\n",
        "\n",
        "        if axis_1 == 0:\n",
        "            acceptable_slides.remove(Move.LEFT)\n",
        "        elif axis_1 == 4:\n",
        "            acceptable_slides.remove(Move.RIGHT)\n",
        "        return acceptable_slides\n",
        "\n",
        "    def get_possible_moves(self):\n",
        "        # __acceptable_slides -> prende from_pos e ritorna le slides possibili.\n",
        "        # for solo sugli element di contorno. e prendiamo le posizion. poi abbiamo acceptable_slides che ci dice le slide possivbili.\n",
        "        moves = []\n",
        "        for row in [0, 4]:\n",
        "            for col in range(5):\n",
        "                if (\n",
        "                    self._board[row, col] == self.current_player_idx\n",
        "                    or self._board[row, col] == -1\n",
        "                ):\n",
        "                    slides = self.__acceptable_slides((row, col))\n",
        "                    for slide in slides:\n",
        "                        moves.append(((col, row), slide))\n",
        "                if (\n",
        "                    self._board[col, row] == self.current_player_idx\n",
        "                    or self._board[col, row] == -1\n",
        "                ):\n",
        "                    slides = self.__acceptable_slides((col, row))\n",
        "                    for slide in slides:\n",
        "                        moves.append(((row, col), slide))\n",
        "        return moves\n",
        "\n",
        "    def play(self, player1: Player, player2: Player) -> int:\n",
        "        self._board = np.full((5, 5), -1, dtype=np.int8)\n",
        "        self.current_player_idx = -1\n",
        "        players = [player1, player2]\n",
        "        winner = -3\n",
        "        n_move = 0\n",
        "        while winner < 0 and n_move < 150:\n",
        "            self.current_player_idx += 1\n",
        "            self.current_player_idx %= len(players)\n",
        "            ok = False\n",
        "            in_loop = 0\n",
        "            while not ok:\n",
        "                in_loop += 1\n",
        "                from_pos, slide = players[self.current_player_idx].make_move(\n",
        "                    self\n",
        "                )\n",
        "                ok = self._Game__move(from_pos, slide, self.current_player_idx)\n",
        "                if in_loop > 200:\n",
        "                    pass\n",
        "            n_move += 1\n",
        "            winner = self.check_winner()\n",
        "        return winner\n",
        "\n",
        "    # look at the problem of the starting position.\n",
        "    # The idea could be to leave the play as it is, then we can make our players play as first then as second.\n",
        "    # we can also make array players shuffle. -> i'll go with this solution\n",
        "    def train(self, trainee: Player, trainer: Player, epochs: int) -> None:\n",
        "        if not trainee.file_name:\n",
        "            print(\"starting full exploration mode\")\n",
        "            trainee.set_epsilon(1)\n",
        "        # if isinstance(trainer, RLayer) and not trainer.file_name:\n",
        "        # trainer.set_epsilon(1)\n",
        "        players = [trainee, trainer]\n",
        "        winning_reward = 1\n",
        "        losing_reward = -3\n",
        "        first_draw_reward = 0.1\n",
        "        second_draw_reward = 0.5\n",
        "        bar = tqdm(total=epochs, desc=\"Epoch\")\n",
        "        for ep in range(epochs):\n",
        "            if ep % (epochs // 30) == 0:\n",
        "                old_eps = trainee.get_epsilon()\n",
        "                new_eps = old_eps - 0.1 if old_eps > 0.3 else old_eps\n",
        "                # eps decrease at the same rate for both.\n",
        "                trainee.set_epsilon(new_eps)\n",
        "                # if isinstance(trainer, RLayer):\n",
        "                # trainer.set_epsilon(new_eps)\n",
        "            np.random.shuffle(players)\n",
        "            winner_idx = self.play(players[0], players[1])\n",
        "            loser_idx = (winner_idx + 1) % 2\n",
        "            if winner_idx != -1:\n",
        "                if isinstance(players[winner_idx], DeepQTrain):\n",
        "                    players[winner_idx].back_prop(winning_reward)\n",
        "                if isinstance(players[loser_idx], DeepQTrain):\n",
        "                    players[loser_idx].back_prop(losing_reward)\n",
        "            else:\n",
        "                if isinstance(players[0], DeepQTrain):\n",
        "                    # if first start draws, not very good.\n",
        "                    players[0].back_prop(first_draw_reward)\n",
        "                if isinstance(players[1], DeepQTrain):\n",
        "                    # if second starting draws, good for him\n",
        "                    players[1].back_prop(second_draw_reward)\n",
        "            bar.update(1)\n",
        "        if isinstance(trainee, DeepQTrain):\n",
        "            trainee.save_policy()\n",
        "        # if isinstance(trainer, RLayer):\n",
        "        #     trainer.save_policy()\n",
        "        return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdASFutklXtq"
      },
      "source": [
        "# Prova"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "iaPcVQFYlZMb"
      },
      "outputs": [],
      "source": [
        "game = GameTrainer()\n",
        "trainee = DeepQTrain({\"learning_rate\": 0.001, \"weight_path\": \"ciao\"})\n",
        "trainer = RandomPlayer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8UX0sZFlfVi",
        "outputId": "f9b1851b-b953-401a-93b5-cae0035fee77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "starting full exploration mode\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch: 100%|██████████| 3000/3000 [04:11<00:00, 11.92it/s]\n"
          ]
        }
      ],
      "source": [
        "game.train(trainee, trainer, 3000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "hSj_t22BHqLe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "starting evaluation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        }
      ],
      "source": [
        "trainee.is_training = False\n",
        "n_game = 1000\n",
        "\n",
        "print(\"starting evaluation\")\n",
        "bar = tqdm(total=n_game * 2, desc=\"Game #\")\n",
        "\n",
        "wins_as_first = 0\n",
        "for _ in range(n_game):\n",
        "    winner = game.play(trainee, RandomPlayer())\n",
        "    if winner == 0:\n",
        "        wins_as_first += 1\n",
        "    bar.update(1)\n",
        "\n",
        "\n",
        "wins_as_second = 0\n",
        "for _ in range(n_game):\n",
        "    winner = game.play(RandomPlayer(), trainee)\n",
        "    if winner == 1:\n",
        "        wins_as_second += 1\n",
        "    bar.update(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Wins as first: 0.77%\n",
            "Wins as second: 0.70%\n",
            "total percentage: 0.74%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        }
      ],
      "source": [
        "print(\"\")\n",
        "print(f\"Wins as first: {wins_as_first/n_game:.2f}%\")\n",
        "print(f\"Wins as second: {wins_as_second/n_game:.2f}%\")\n",
        "\n",
        "print(f\"total percentage: {(wins_as_first + wins_as_second)/(n_game*2):.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "HhRzUwn8QNNl",
        "SR9Il94jQgEC"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
