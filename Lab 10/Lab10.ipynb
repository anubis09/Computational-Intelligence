{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "* Submission: [Dies Natalis Solis Invicti](https://en.wikipedia.org/wiki/Sol_Invictus)\n",
    "* Reviews: [Befana](https://en.wikipedia.org/wiki/Befana)\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Reviews will be assigned  on Monday, December 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "from termcolor import colored\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class Player(ABC):\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"You can change this for your player if you need to handle state/have memory\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def decide_move(\n",
    "        self, game: \"TicTacToe\", move_symbol: int\n",
    "    ) -> tuple[int, int]:\n",
    "        \"\"\"\n",
    "        game: the TicTacToe game. You can use it to override the current game with yours, but everything is evaluated by the main game\n",
    "        return values: this method shall return a tuple of X,Y positions\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def set_train_mode(self, mode: bool):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def back_prop(self, reward: int):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "EPSILON = 0.3\n",
    "LR = 0.2\n",
    "GAMMA_DECAY = 0.9\n",
    "\n",
    "\n",
    "class RLayer(Player):\n",
    "    def __init__(\n",
    "        self, path: str = None, epsilon: float = EPSILON, name: str = \"\"\n",
    "    ):\n",
    "        self.game_state = []\n",
    "        self.path = path\n",
    "        self.name = name\n",
    "        self.epsilon = epsilon\n",
    "        self.lr = LR\n",
    "        self.gamma_decay = GAMMA_DECAY\n",
    "        self.train_mode = False  # if set to true allows move exploration.\n",
    "        self.policy = defaultdict(float)\n",
    "        if path:\n",
    "            f = open(path, \"r\")\n",
    "            policy = dict(json.load(f))\n",
    "            for k, v in policy.items():\n",
    "                self.policy[k] = v\n",
    "\n",
    "    def set_train_mode(self, mode: bool):\n",
    "        self.train_mode = mode\n",
    "\n",
    "    def decide_move(\n",
    "        self, game: \"TicTacToe\", move_symbol: int\n",
    "    ) -> tuple[int, int]:\n",
    "        \"\"\"\n",
    "        The following function is used by an agent to determine its next move in the game.\n",
    "        During training only:\n",
    "            with a probability epsilon, it randomly selects a move from all possible moves,\n",
    "            otherwise selects the move with the highest predicted policy value based on the agent's learned policy.\n",
    "        The function returns the chosen move.\n",
    "        \"\"\"\n",
    "        all_moves = game.possible_moves()\n",
    "        if self.train_mode and np.random.random() < self.epsilon:\n",
    "            # exploration phase\n",
    "            index = np.random.choice(len(all_moves))\n",
    "            move = all_moves[index]\n",
    "            board_next_move = deepcopy(game)\n",
    "            board_next_move.make_move(move, move_symbol)\n",
    "            self.game_state.append(board_next_move.hash())\n",
    "            return move\n",
    "        else:\n",
    "            # selects the best move in the policy.\n",
    "            best_move = None\n",
    "            best_hash = None\n",
    "            best_value = float(\"-inf\")\n",
    "            for move in all_moves:\n",
    "                board_next_move = deepcopy(game)\n",
    "                board_next_move.make_move(move, move_symbol)\n",
    "                hash = board_next_move.hash()\n",
    "                if self.policy[hash] > best_value:\n",
    "                    best_move = move\n",
    "                    best_hash = hash\n",
    "                    best_value = self.policy[hash]\n",
    "            if self.train_mode:\n",
    "                self.game_state.append(best_hash)\n",
    "            return best_move\n",
    "\n",
    "    def back_prop(self, reward: int):\n",
    "        \"\"\"\n",
    "        The back_prop function is responsible for updating the policy values in the agent's memory during the training process.\n",
    "        It adjusts the policy values based on the received reward.\n",
    "        \"\"\"\n",
    "        for state in reversed(self.game_state):\n",
    "            # state is an hash of next_board\n",
    "            self.policy[state] += self.lr * (\n",
    "                self.gamma_decay * reward - self.policy[state]\n",
    "            )\n",
    "            reward = self.policy[state]\n",
    "        self.game_state = []\n",
    "\n",
    "    def save_policy(self):\n",
    "        \"\"\"\n",
    "        It saves the policy learnt when trining is done\n",
    "        \"\"\"\n",
    "        if not self.path:\n",
    "            s = \"policy\"\n",
    "            if self.name != \"\":\n",
    "                s += \"_\" + self.name\n",
    "            self.path = f\"{s}.json\"\n",
    "        f = open(self.path, \"w\")\n",
    "        json.dump(self.policy, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPlayer(Player):\n",
    "    \"\"\"\n",
    "    RandomPlayer just choose a ranodm move among the possible ones in a given board configuration\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def set_train_mode(self, mode: bool):\n",
    "        return super().set_train_mode(mode)\n",
    "\n",
    "    def decide_move(self, game: \"TicTacToe\", move_symbol: int) -> str:\n",
    "        all_moves = game.possible_moves()\n",
    "        index = np.random.choice(len(all_moves))\n",
    "        move = all_moves[index]\n",
    "        return move\n",
    "\n",
    "    def back_prop(self, reward: int):\n",
    "        return super().back_prop(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanPlayer(Player):\n",
    "    \"\"\"\n",
    "    HumanPlayer allows a user to play against our trained RLayer player\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def set_train_mode(self, mode: bool):\n",
    "        return super().set_train_mode(mode)\n",
    "\n",
    "    def decide_move(\n",
    "        self, game: \"TicTacToe\", move_symbol: int\n",
    "    ) -> tuple[int, int]:\n",
    "        # printing with the assigned color, for clarity.\n",
    "        color = \"green\" if move_symbol == -1 else \"red\"\n",
    "        print_string = \"Human player\"\n",
    "        print(f\"\\n{colored(print_string,color)} it's your turn\")\n",
    "        move = input(\"Insert x and y coordinates in format: 'x y' :\")\n",
    "        x, y = move.split(\" \")\n",
    "        return (int(x), int(y))\n",
    "\n",
    "    def back_prop(self, reward: int):\n",
    "        return super().back_prop(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TicTacToe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self, board=None):\n",
    "        \"\"\"\n",
    "        Board legend:\n",
    "        cell = 0 -> no move on this cell\n",
    "        cell = 1 -> player 1 made 'x' (1) as move\n",
    "        cell = -1 -> player 2 made 'o' (-1) as move\n",
    "        \"\"\"\n",
    "        self.train_mode = False\n",
    "        if isinstance(board, np.ndarray):\n",
    "            self.board = board\n",
    "        else:  # all cells initalized with 0\n",
    "            self.board = np.zeros(shape=(3, 3))\n",
    "\n",
    "    def hash(self) -> str:\n",
    "        return str(self.board)\n",
    "\n",
    "    def check_win(self):\n",
    "        \"\"\"Checks if someone won the game.\"\"\"\n",
    "\n",
    "        # Check win by columns\n",
    "        if (abs(self.board.sum(axis=0)) == 3).any():\n",
    "            return True\n",
    "        # Check win by rows\n",
    "        if (abs(self.board.sum(axis=1)) == 3).any():\n",
    "            return True\n",
    "\n",
    "        # Check win by diagonals\n",
    "        sum_diag_princ = 0\n",
    "        sum_diag_back = 0\n",
    "        for i in range(3):\n",
    "            sum_diag_back += self.board[i][i]\n",
    "            sum_diag_princ += self.board[2 - i][i]\n",
    "        if abs(sum_diag_back) == 3 or abs(sum_diag_princ) == 3:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def check_tie(self) -> bool:\n",
    "        \"\"\"Checks if there is a tie.\"\"\"\n",
    "        if not self.possible_moves():\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def make_move(self, move: tuple[int, int], value: int):\n",
    "        \"\"\"Take a move in format x y and makes it.\"\"\"\n",
    "        x, y = move\n",
    "        # check is a valid move\n",
    "        if not (0 <= x <= 2 and 0 <= y <= 2):\n",
    "            print(\"invalid move\")\n",
    "        elif self.board[x][y] != 0:\n",
    "            print(\"invalid move\")\n",
    "        else:\n",
    "            self.board[x][y] = value\n",
    "\n",
    "    def possible_moves(self):\n",
    "        \"\"\"Return all the possible available moves to make.\"\"\"\n",
    "        moves = []\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                if self.board[i][j] == 0:\n",
    "                    moves.append((i, j))\n",
    "        return moves\n",
    "\n",
    "    def print(self):\n",
    "        \"\"\"Pretty printing of the board.\"\"\"\n",
    "        l_len = 19\n",
    "        clear_output()\n",
    "        print(\"   \", \"  (0)   (1)   (2)\")\n",
    "        for i in range(3):\n",
    "            print(\"   \", \"-\" * l_len)\n",
    "            print(f\"({i})\", \"| \", end=\"\")\n",
    "            print(\n",
    "                \" | \".join(\n",
    "                    map(\n",
    "                        lambda e: \"   \"\n",
    "                        if e == 0\n",
    "                        else colored(\" X \", \"red\")\n",
    "                        if e == 1\n",
    "                        else colored(\" O \", \"green\"),\n",
    "                        self.board[i].astype(int),\n",
    "                    )\n",
    "                ),\n",
    "                end=\" \",\n",
    "            )\n",
    "            print(\"|\")\n",
    "        print(\"   \", \"-\" * l_len)\n",
    "\n",
    "    def run(\n",
    "        self, player1: \"Player\", player2: \"Player\"\n",
    "    ) -> tuple[int, (\"Player\", \"Player\")]:\n",
    "        \"\"\"\n",
    "        This function just plays a game between player1 and player2.\n",
    "        It returns the the winner index 0/1 or -1 if it's tie.\n",
    "        \"\"\"\n",
    "        someone_won, is_tie = False, False\n",
    "        # always reset the board state.\n",
    "        self.board = np.zeros(shape=(3, 3))\n",
    "        players = [player1, player2]\n",
    "        # this is the index of the second starting player.\n",
    "        pl_index = np.random.choice(len(players))\n",
    "        # value of the second starting player\n",
    "        if not self.train_mode:\n",
    "            self.print()\n",
    "        # The starting player will have 1 as value to assign.\n",
    "        value_to_assign = -1\n",
    "        while not someone_won and not is_tie:\n",
    "            pl_index += 1\n",
    "            pl_index %= 2\n",
    "            value_to_assign *= -1\n",
    "            move = players[pl_index].decide_move(self, value_to_assign)\n",
    "            self.make_move(move, value_to_assign)\n",
    "            if not self.train_mode:\n",
    "                self.print()\n",
    "            someone_won = self.check_win()\n",
    "            is_tie = self.check_tie()\n",
    "\n",
    "        if is_tie and not someone_won:\n",
    "            pl_index = -1\n",
    "        return (pl_index, players)\n",
    "\n",
    "    def train(self, player1: Player, trainers: list[Player], epochs: int):\n",
    "        \"\"\"\n",
    "        The train function iterates through a specified number of epochs, randomly selecting trainers for each iteration.\n",
    "        After a game is played between the primary player (player1) and the selected trainer, back_prop is called for reward updates.\n",
    "        The function also saves the learned policy for RLayer player (Reinforcement Learning player)\n",
    "        \"\"\"\n",
    "        WIN_REWARD = 1\n",
    "        TIES_REWARD = 0.5\n",
    "        LOSE_REWARD = -1\n",
    "        self.train_mode = True\n",
    "        player1.set_train_mode(True)\n",
    "        for trainer in trainers:  # we may have more than one trainer.\n",
    "            trainer.set_train_mode(True)\n",
    "        bar = tqdm(total=epochs, desc=\"Epoch\")\n",
    "        for _ in range(epochs):\n",
    "            idx_pl = np.random.choice(len(trainers))\n",
    "            trainer = trainers[idx_pl]\n",
    "            winner_idx, players = self.run(player1, trainer)\n",
    "            if winner_idx >= 0:\n",
    "                players[winner_idx].back_prop(WIN_REWARD)\n",
    "                players[(winner_idx + 1) % 2].back_prop(LOSE_REWARD)\n",
    "            else:\n",
    "                players[0].back_prop(TIES_REWARD)\n",
    "                players[1].back_prop(TIES_REWARD)\n",
    "            bar.update(1)\n",
    "        self.train_mode = False\n",
    "        if isinstance(player1, RLayer):\n",
    "            player1.save_policy()\n",
    "            player1.set_train_mode(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def player_evaluation(player_to_eval, player_to_fight, num_game):\n",
    "    \"\"\"\n",
    "    Given a trained player in input and an otehr player to play against,\n",
    "    this function evaulates the first player (player_to_eval) on 'num_game' games.\n",
    "    It prints its %wins, %ties and %loss\"\"\"\n",
    "    wins = 0\n",
    "    ties = 0\n",
    "    game = TicTacToe()\n",
    "    for _ in range(num_game):\n",
    "        win_idx, players = game.run(player_to_eval, player_to_fight)\n",
    "        if win_idx != -1 and id(players[win_idx]) == id(player_to_eval):\n",
    "            wins += 1\n",
    "        elif win_idx == -1:\n",
    "            ties += 1\n",
    "    print()\n",
    "    print(f\"% of wins of RL: {wins/num_game:.2%}\")\n",
    "    print(f\"% of ties: {ties/num_game:.2%}\")\n",
    "    print(f\"(% of wins other player: {1 - ((ties + wins)/num_game):.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_GAMES_EVALUATION = 2000\n",
    "NUM_EPOCHS_TRAINING = 400_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training RL player vs. RandomPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_base = RLayer(name=\"rl_base\")\n",
    "random_player = RandomPlayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 400000/400000 [07:28<00:00, 891.74it/s]\n"
     ]
    }
   ],
   "source": [
    "game = TicTacToe()\n",
    "game.train(rl_base, [random_player], NUM_EPOCHS_TRAINING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      (0)   (1)   (2)\n",
      "    -------------------\n",
      "(0) |     | \u001b[32m O \u001b[0m | \u001b[31m X \u001b[0m |\n",
      "    -------------------\n",
      "(1) |     | \u001b[31m X \u001b[0m |     |\n",
      "    -------------------\n",
      "(2) | \u001b[31m X \u001b[0m |     | \u001b[32m O \u001b[0m |\n",
      "    -------------------\n",
      "\n",
      "% of wins of RL: 92.90%\n",
      "% of ties: 5.70%\n",
      "(% of wins other player: 1.40%)\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model against a random player\n",
    "\n",
    "player_evaluation(rl_base, random_player, NUM_GAMES_EVALUATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training RL player vs other RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_RL_trained = RLayer(name=\"rl_RL_trained\")\n",
    "rl_trainer = RLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 400000/400000 [14:24<00:00, 462.80it/s]\n"
     ]
    }
   ],
   "source": [
    "game = TicTacToe()\n",
    "game.train(rl_RL_trained, [rl_trainer], NUM_EPOCHS_TRAINING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      (0)   (1)   (2)\n",
      "    -------------------\n",
      "(0) | \u001b[31m X \u001b[0m | \u001b[32m O \u001b[0m | \u001b[31m X \u001b[0m |\n",
      "    -------------------\n",
      "(1) | \u001b[32m O \u001b[0m | \u001b[31m X \u001b[0m | \u001b[32m O \u001b[0m |\n",
      "    -------------------\n",
      "(2) |     |     | \u001b[31m X \u001b[0m |\n",
      "    -------------------\n",
      "\n",
      "% of wins of RL: 89.40%\n",
      "% of ties: 9.45%\n",
      "(% of wins other player: 1.15%)\n"
     ]
    }
   ],
   "source": [
    "player_evaluation(rl_RL_trained, random_player, NUM_GAMES_EVALUATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training RL player vs other RL player and RandomPlayer together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "board = TicTacToe()\n",
    "the_ROCK = RLayer(name=\"the_ROCK\")\n",
    "trainer_1 = rl_RL_trained\n",
    "trainer_2 = random_player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 400000/400000 [10:52<00:00, 613.48it/s]\n"
     ]
    }
   ],
   "source": [
    "board.train(the_ROCK, [trainer_1, trainer_2], NUM_EPOCHS_TRAINING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      (0)   (1)   (2)\n",
      "    -------------------\n",
      "(0) | \u001b[31m X \u001b[0m | \u001b[32m O \u001b[0m |     |\n",
      "    -------------------\n",
      "(1) | \u001b[32m O \u001b[0m | \u001b[31m X \u001b[0m |     |\n",
      "    -------------------\n",
      "(2) |     |     | \u001b[31m X \u001b[0m |\n",
      "    -------------------\n",
      "\n",
      "% of wins of RL: 93.00%\n",
      "% of ties: 6.50%\n",
      "(% of wins other player: 0.50%)\n"
     ]
    }
   ],
   "source": [
    "player_evaluation(the_ROCK, random_player, NUM_GAMES_EVALUATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Player match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Run cells below if you want to play a match aginst a trained player*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = TicTacToe()\n",
    "human_player = HumanPlayer()\n",
    "# choose your favorite opponent:\n",
    "#   rl_baseline     rl_RL_trained     random_player    the_ROCK\n",
    "opponent = rl_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_winner(winner: int, players: tuple) -> None:\n",
    "    if winner == -1:\n",
    "        print(f\"\\n🟡 Tie\")\n",
    "    elif (winner == 0 and isinstance(players[0], HumanPlayer)) or (\n",
    "        winner == 1 and isinstance(players[1], HumanPlayer)\n",
    "    ):\n",
    "        print(f\"\\n🟢 You won\")\n",
    "    else:\n",
    "        print(f\"\\n🔴 You lost. {type(players[winner]).__name__} won,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      (0)   (1)   (2)\n",
      "    -------------------\n",
      "(0) | \u001b[32m O \u001b[0m | \u001b[32m O \u001b[0m | \u001b[31m X \u001b[0m |\n",
      "    -------------------\n",
      "(1) | \u001b[31m X \u001b[0m | \u001b[31m X \u001b[0m | \u001b[32m O \u001b[0m |\n",
      "    -------------------\n",
      "(2) | \u001b[32m O \u001b[0m | \u001b[31m X \u001b[0m | \u001b[31m X \u001b[0m |\n",
      "    -------------------\n",
      "\n",
      "🟡 Tie\n"
     ]
    }
   ],
   "source": [
    "winner, players = game.run(human_player, opponent)\n",
    "print_winner(winner, players)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
