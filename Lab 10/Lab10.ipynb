{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "* Submission: [Dies Natalis Solis Invicti](https://en.wikipedia.org/wiki/Sol_Invictus)\n",
    "* Reviews: [Befana](https://en.wikipedia.org/wiki/Befana)\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Reviews will be assigned  on Monday, December 4\n",
    "* You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "from termcolor import colored\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class Player(ABC):\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"You can change this for your player if you need to handle state/have memory\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def decide_move(\n",
    "        self, game: \"TicTacToe\", move_symbol: int\n",
    "    ) -> tuple[int, int]:\n",
    "        \"\"\"\n",
    "        game: the TicTacToe game. You can use it to override the current game with yours, but everything is evaluated by the main game\n",
    "        return values: this method shall return a tuple of X,Y positions\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def back_prop(self, reward: int):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "\n",
    "WIN_REWARD = 1\n",
    "LOSE_REWARD = -1\n",
    "TIE_REWARD = 0.2\n",
    "\n",
    "EPSILON = 0.3\n",
    "LR = 0.2\n",
    "GAMMA_DECAY = 0.9\n",
    "\n",
    "\n",
    "class RLayer(Player):\n",
    "    def __init__(self, path: str = None, epsilon: float = EPSILON):\n",
    "        self.game_state = []\n",
    "        self.path = path\n",
    "        self.epsilon = epsilon\n",
    "        self.lr = LR\n",
    "        self.gamma_decay = GAMMA_DECAY\n",
    "        self.train_mode = False\n",
    "        self.policy = defaultdict(float)\n",
    "        if path:\n",
    "            f = open(path, \"r\")\n",
    "            policy = dict(json.load(f))\n",
    "            for k, v in policy.items():\n",
    "                self.policy[k] = v\n",
    "\n",
    "    def set_train_mode(self, mode: bool):\n",
    "        self.train_mode = mode\n",
    "\n",
    "    def decide_move(\n",
    "        self, game: \"TicTacToe\", move_symbol: int\n",
    "    ) -> tuple[int, int]:\n",
    "        all_moves = game.possible_moves()\n",
    "        if self.train_mode and np.random.random() < self.epsilon:\n",
    "            # exploration phase\n",
    "            index = np.random.randint(len(all_moves))\n",
    "            return all_moves[index]\n",
    "        else:\n",
    "            best_move = None\n",
    "            best_hash = None\n",
    "            best_value = float(\"-inf\")\n",
    "            for move in all_moves:\n",
    "                board_next_move = deepcopy(game)\n",
    "                board_next_move.make_move(move, move_symbol)\n",
    "                hash = board_next_move.hash()\n",
    "                if self.policy[hash] > best_value:\n",
    "                    best_move = move\n",
    "                    best_hash = hash\n",
    "                    best_value = self.policy[hash]\n",
    "            self.game_state.append(best_hash)\n",
    "            return best_move\n",
    "\n",
    "    def back_prop(self, reward: int):\n",
    "        for state in reversed(self.game_state):\n",
    "            # state is an hash of next_board\n",
    "            self.policy[state] += self.lr * (\n",
    "                self.gamma_decay * reward - self.policy[state]\n",
    "            )\n",
    "            reward = self.policy[state]\n",
    "        self.game_state = []\n",
    "\n",
    "    def save_policy(self):\n",
    "        if not self.path:\n",
    "            self.path = \"policy_lore.json\"\n",
    "        f = open(self.path, \"w\")\n",
    "        json.dump(self.policy, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self, board=None):\n",
    "        \"\"\"\n",
    "        Board legend:\n",
    "        cell = 0 -> no move on this cell\n",
    "        cell = 1 -> player 1 made 'x' (1) as move\n",
    "        cell = -1 -> player 2 made 'o' (-1) as move\n",
    "        \"\"\"\n",
    "        self.train_mode = False\n",
    "        if isinstance(board, np.ndarray):\n",
    "            self.board = board\n",
    "        else:  # all cells initalized at 0\n",
    "            self.board = np.zeros(shape=(3, 3))\n",
    "\n",
    "    def hash(self) -> str:\n",
    "        return str(self.board)\n",
    "\n",
    "    def check_win(self):\n",
    "        \"\"\"Checks if someone won the game.\"\"\"\n",
    "\n",
    "        # Check win by columns\n",
    "        if (abs(self.board.sum(axis=0)) == 3).any():\n",
    "            return True\n",
    "        # Check win by rows\n",
    "        if (abs(self.board.sum(axis=1)) == 3).any():\n",
    "            return True\n",
    "\n",
    "        # Check win by diagonals\n",
    "        sum_diag_princ = 0\n",
    "        sum_diag_back = 0\n",
    "        for i in range(3):\n",
    "            sum_diag_back += self.board[i][i]\n",
    "            sum_diag_princ += self.board[2 - i][i]\n",
    "        if abs(sum_diag_back) == 3 or abs(sum_diag_princ) == 3:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def check_tie(self) -> bool:\n",
    "        if not self.possible_moves():\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def make_move(self, move: tuple[int, int], value: int):\n",
    "        \"\"\"Take a move in format x y and makes it.\"\"\"\n",
    "        x, y = move\n",
    "        # check is a valid move\n",
    "        if not (0 <= x <= 2 and 0 <= y <= 2):\n",
    "            print(\"invalid move\")\n",
    "        elif self.board[x][y] != 0:\n",
    "            print(\"invalid move\")\n",
    "        else:\n",
    "            self.board[x][y] = value\n",
    "\n",
    "    def possible_moves(self):\n",
    "        \"\"\"Return all the possible available moves to make.\"\"\"\n",
    "        moves = []\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                if self.board[i][j] == 0:\n",
    "                    moves.append((i, j))\n",
    "        return moves\n",
    "\n",
    "    def print(self):\n",
    "        l_len = 19\n",
    "        clear_output()\n",
    "        print(f\"{colored('Player 1', 'green')} make your move:\\n\")\n",
    "        print(\"   \", \"  (0)   (1)   (2)\")\n",
    "        for i in range(3):\n",
    "            print(\"   \", \"-\" * l_len)\n",
    "            print(f\"({i})\", \"| \", end=\"\")\n",
    "            print(\n",
    "                \" | \".join(\n",
    "                    map(\n",
    "                        lambda e: \"   \"\n",
    "                        if e == 0\n",
    "                        else colored(\" X \", \"red\")\n",
    "                        if e == 1\n",
    "                        else colored(\" O \", \"green\"),\n",
    "                        self.board[i].astype(int),\n",
    "                    )\n",
    "                ),\n",
    "                end=\" \",\n",
    "            )\n",
    "            print(\"|\")\n",
    "        print(\"   \", \"-\" * l_len)\n",
    "\n",
    "    def run(\n",
    "        self, player1: Player, player2: Player\n",
    "    ) -> tuple[int, (Player, Player)]:\n",
    "        \"\"\"Returns a int with the winning player index. If tie, the index value will be -1.\"\"\"\n",
    "        board_state = []\n",
    "        someone_won, is_tie = False, False\n",
    "        self.board = np.zeros(shape=(3, 3))\n",
    "        players = [player1, player2]\n",
    "        np.random.shuffle(players)\n",
    "        pl_index = -1\n",
    "        value_to_assign = -1 if isinstance(players[0], RLayer) else 1\n",
    "        if not self.train_mode:\n",
    "            self.print()\n",
    "        while not someone_won and not is_tie:\n",
    "            pl_index += 1\n",
    "            pl_index %= 2\n",
    "            value_to_assign *= -1\n",
    "            move = players[pl_index].decide_move(self, value_to_assign)\n",
    "            self.make_move(move, value_to_assign)\n",
    "            if not self.train_mode:\n",
    "                self.print()\n",
    "            someone_won = self.check_win()\n",
    "            is_tie = self.check_tie()\n",
    "\n",
    "        if is_tie and not someone_won:\n",
    "            pl_index = -1\n",
    "        return (pl_index, players)\n",
    "\n",
    "    def train(self, player1: Player, players: list, epochs: int):\n",
    "        self.train_mode = True\n",
    "        bar = tqdm(total=epochs, desc=\"Epoch\")\n",
    "        if isinstance(player1, RLayer):\n",
    "            player1.set_train_mode = True\n",
    "        for _ in range(epochs):\n",
    "            index = np.random.randint(len(players))\n",
    "            player2 = players[index]\n",
    "            winner_idx, players = self.run(player1, player2)\n",
    "            if winner_idx >= 0:\n",
    "                players[winner_idx].back_prop(WIN_REWARD)\n",
    "                players[(winner_idx + 1) % 2].back_prop(LOSE_REWARD)\n",
    "            else:\n",
    "                players[0].back_prop(TIE_REWARD)\n",
    "                players[1].back_prop(TIE_REWARD)\n",
    "            bar.update(1)\n",
    "        if isinstance(player1, RLayer):\n",
    "            player1.save_policy()\n",
    "            player1.set_train_mode = False\n",
    "        self.train_mode = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPlayer(Player):\n",
    "    def __init__(self) -> None:\n",
    "        self.pol = defaultdict(float)\n",
    "        super().__init__()\n",
    "\n",
    "    def decide_move(self, game: \"TicTacToe\", move_symbol: int) -> str:\n",
    "        all_moves = game.possible_moves()\n",
    "        index = np.random.choice(len(all_moves))\n",
    "        move = all_moves[index]\n",
    "        board = deepcopy(game)\n",
    "        board.make_move(move, move_symbol)\n",
    "        self.pol[board.hash] += 1\n",
    "        return all_moves[index]\n",
    "        # return super().make_move(game)\n",
    "\n",
    "    def back_prop(self, reward: int):\n",
    "        return super().back_prop(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanPlayer(Player):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def decide_move(\n",
    "        self, game: \"TicTacToe\", move_symbol: int\n",
    "    ) -> tuple[int, int]:\n",
    "        move = input(\"Insert x and y coordinates in format: 'x y' :\")\n",
    "        x, y = move.split(\" \")\n",
    "        return (int(x), int(y))\n",
    "\n",
    "    def back_prop(self, reward: int):\n",
    "        return super().back_prop(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nuovo training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp = RandomPlayer()\n",
    "rl1_new_train = RLayer()\n",
    "rl2_new_train = RLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 1000000/1000000 [43:59<00:00, 378.92it/s]  \n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1_000_000\n",
    "game = TicTacToe()\n",
    "game.train(rl1_new_train, [rp], EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered states rl2: 916/19683 (4.65%)\n",
      "\tDifferent from 0: 8/916 (0.87%)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Discovered states rl2: {len(rl1_new_train.policy)}/{3**9} ({len(rl1_new_train.policy)/3**9:.2%})\"\n",
    ")\n",
    "ll = len([t for t in rl1_new_train.policy.items() if t[1] > 0])\n",
    "print(\n",
    "    f\"\\tDifferent from 0: {ll}/{len(rl1_new_train.policy)} ({ll/len(rl1_new_train.policy):.2%})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mPlayer 1\u001b[0m make your move:\n",
      "\n",
      "      (0)   (1)   (2)\n",
      "    -------------------\n",
      "(0) | \u001b[31m X \u001b[0m | \u001b[31m X \u001b[0m | \u001b[32m O \u001b[0m |\n",
      "    -------------------\n",
      "(1) | \u001b[31m X \u001b[0m | \u001b[32m O \u001b[0m | \u001b[32m O \u001b[0m |\n",
      "    -------------------\n",
      "(2) | \u001b[32m O \u001b[0m |     |     |\n",
      "    -------------------\n",
      "\n",
      "% of wins of RL: 61.20%\n",
      "% of ties: 3.90%\n",
      "(% of wins other player: 34.90%)\n"
     ]
    }
   ],
   "source": [
    "NUM_GAME = 1000\n",
    "win = 0\n",
    "ties = 0\n",
    "rl1_new_train = RLayer(path=\"policy.json\")\n",
    "for _ in range(NUM_GAME):\n",
    "    win_idx, players = game.run(rp, rl1_new_train)\n",
    "    if win_idx != -1 and id(players[win_idx]) == id(rl1_new_train):\n",
    "        win += 1\n",
    "    elif win_idx == -1:\n",
    "        ties += 1\n",
    "print()\n",
    "print(f\"% of wins of RL: {win/NUM_GAME:.2%}\")\n",
    "print(f\"% of ties: {ties/NUM_GAME:.2%}\")\n",
    "print(f\"(% of wins other player: {1 - ((ties + win)/NUM_GAME):.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = HumanPlayer()\n",
    "game.run(rl1_new_train, hp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roba Lore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp = RandomPlayer()\n",
    "rl1 = RLayer()\n",
    "# rl2 = RLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 100000/100000 [02:57<00:00, 563.45it/s]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100_000\n",
    "game = TicTacToe()\n",
    "game.train(rl1, [rp], EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered states rl: 1045/2907 (35.95%)\n",
      "Discovered states rp: 9829/2907 (338.11%)\n"
     ]
    }
   ],
   "source": [
    "# analysis\n",
    "from itertools import product\n",
    "\n",
    "# Define the possible values\n",
    "values = [0, 1, -1]\n",
    "\n",
    "# Generate all possible combinations\n",
    "combinations = product(values, repeat=9)\n",
    "\n",
    "\n",
    "# Function to check if matrix meets the condition\n",
    "def meets_condition(matrix):\n",
    "    return np.sum(matrix == 1) == (np.sum(matrix == -1) + 1)\n",
    "\n",
    "\n",
    "# Convert combinations to 3x3 matrices and print\n",
    "counter = 0\n",
    "for combo in combinations:\n",
    "    matrix = np.array(combo).reshape(3, 3)\n",
    "    if meets_condition(matrix):\n",
    "        counter += 1\n",
    "MAX_DISCOVERABLE_STATES = counter\n",
    "\n",
    "print(\n",
    "    f\"Discovered states rl: {len(rl1.policy)}/{MAX_DISCOVERABLE_STATES} ({len(rl1.policy)/MAX_DISCOVERABLE_STATES:.2%})\"\n",
    ")\n",
    "print(\n",
    "    f\"Discovered states rp: {len(rp.pol)}/{MAX_DISCOVERABLE_STATES} ({len(rp.pol)/MAX_DISCOVERABLE_STATES:.2%})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mPlayer 1\u001b[0m make your move:\n",
      "\n",
      "      (0)   (1)   (2)\n",
      "    -------------------\n",
      "(0) | \u001b[31m X \u001b[0m | \u001b[31m X \u001b[0m | \u001b[32m O \u001b[0m |\n",
      "    -------------------\n",
      "(1) | \u001b[31m X \u001b[0m |     |     |\n",
      "    -------------------\n",
      "(2) | \u001b[32m O \u001b[0m | \u001b[32m O \u001b[0m | \u001b[32m O \u001b[0m |\n",
      "    -------------------\n",
      "\n",
      "% of wins of RL: 58.60%\n",
      "% of ties: 4.20%\n",
      "(% of wins other player: 37.20%)\n"
     ]
    }
   ],
   "source": [
    "NUM_GAME = 1000\n",
    "win_rl1 = 0\n",
    "ties = 0\n",
    "for _ in range(NUM_GAME):\n",
    "    win_idx, players = game.run(rl1, rp)\n",
    "    if win_idx != -1 and id(players[win_idx]) == id(rl1):\n",
    "        win_rl1 += 1\n",
    "    elif win_idx == -1:\n",
    "        ties += 1\n",
    "print()\n",
    "print(f\"% of wins of RL: {win_rl1/NUM_GAME:.2%}\")\n",
    "print(f\"% of ties: {ties/NUM_GAME:.2%}\")\n",
    "print(f\"(% of wins other player: {1 - ((ties + win_rl1)/NUM_GAME):.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mPlayer 1\u001b[0m make your move:\n",
      "\n",
      "      (0)   (1)   (2)\n",
      "    -------------------\n",
      "(0) |     | \u001b[32m O \u001b[0m |     |\n",
      "    -------------------\n",
      "(1) |     |     |     |\n",
      "    -------------------\n",
      "(2) |     |     |     |\n",
      "    -------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m hp \u001b[38;5;241m=\u001b[39m HumanPlayer()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrp\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 105\u001b[0m, in \u001b[0;36mTicTacToe.run\u001b[0;34m(self, player1, player2)\u001b[0m\n\u001b[1;32m    103\u001b[0m pl_index \u001b[38;5;241m%\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    104\u001b[0m value_to_assign \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 105\u001b[0m move \u001b[38;5;241m=\u001b[39m \u001b[43mplayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpl_index\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecide_move\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_to_assign\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_move(move, value_to_assign)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_mode:\n",
      "Cell \u001b[0;32mIn[29], line 9\u001b[0m, in \u001b[0;36mHumanPlayer.decide_move\u001b[0;34m(self, game, move_symbol)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecide_move\u001b[39m(\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m, game: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTicTacToe\u001b[39m\u001b[38;5;124m\"\u001b[39m, move_symbol: \u001b[38;5;28mint\u001b[39m\n\u001b[1;32m      7\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m      8\u001b[0m     move \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInsert x and y coordinates in format: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx y\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m :\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m move\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mint\u001b[39m(x), \u001b[38;5;28mint\u001b[39m(y))\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "hp = HumanPlayer()\n",
    "game.run(hp, rp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-fLJ3OwGs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
